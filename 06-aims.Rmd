--- 
knit: "bookdown::preview_chapter"
---

# MODULE 2 Define Your Study Aims {-}

By the end of this module, you should be able to:

* Ask a good research question and develop study aims and hypotheses 
* Develop a theory of change and logic model
* Identify indicators to measure throughout the causal chain

# Research Questions and Aims {#aims}

```{marginfigure}
<iframe src="https://giphy.com/embed/82okbIOv8krUhwgHCz" width="300" height="169" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p>Give yourself a pat on the back. Go ahead. No one is watching. <a href="https://giphy.com/gifs/CMNHospitals-childrens-miracle-network-hospitals-kid-pediatric-patients-82okbIOv8krUhwgHCz">via GIPHY</a></p>
```

If you're following the plan I've outlined so far, you've been seeking out research ideas by attending presentations, talking with fellow students and mentors, skimming interesting journals, searching research databases like *PubMed* for keywords you've identified, and finding relevant systematic reviews and meta-analyses. You've also been developing your critical appraisal skills and, in the process, have been taking note of gaps in our knowledge. All of this work is leading you to identify potential research problems worth solving. The next step is to take a broad research problem and narrow your focus to a more specific research question and develop study aims and (potentially) hypotheses. I'll walk you through the process and conclude the chapter with recommendations for how to write about these ideas in a proposal or manuscript.

## Types of research questions

There are three basic types of research questions we can ask [@hernan2019]:

1. Descriptive
2. Predictive/Relational
3. Causal (counterfactual prediction)

This framework is an attempt to simplify the world to help you learn, but you will soon see that the lines between these three categories can blur. For one, a study that aims to assess the evidence for a claim that `X` causes `Y` can include elements of prediction and description. Second, answering questions of all three types can involve statistical inference, as we often want to quantify the uncertainty in our estimates. So there is a possibility of conflating our aims (e.g., to estimate the causal effect of `X` on `Y`) and methods (e.g., the use of a statistical test to examine the association—a relationship—between `X` and `Y`) [@hernan2018]. Nevertheless, it is helpful to erect some boundaries to introduce these concepts and let you decide if they are useful as you gain more expertise. 

### DESCRIPTIVE

Every study uses an element of **description**. Let's say you recruit a sample of 100 people who suffer from the same disorder and conduct a trial to estimate the effect of a new drug on some clinical outcome. When you summarize what you know about these 100 people at the time they were recruited, for instance the average age of the group, you're describing the sample. Descriptive summaries appear in nearly every research article. But we can distinguish between the use of descriptive statistics—e.g., what is the mean age of these 100 people, the sample—and descriptive research questions. 

One common descriptive research question in global health follows this format:

> What percentage of women of reproductive age in Nepal use a modern method of contraception?

As we will discuss later in the book, you could answer this question by conducting a survey of contraceptive behavior with a representative sample of women in Nepal. That's what the DHS Program did in 2010 [@dhsnepal2011]. 

```{marginfigure}
"[Modern methods](https://www.who.int/news-room/fact-sheets/detail/family-planning-contraception)" like condoms, implants, pills, etc, are distinguished from (and are more effective than) "traditional methods" such as withdrawal and the rhythm method.
```

Researchers surveyed a random sample of 10,826 households across the country and interviewed 12,674 women between the ages of 15 and 49 about their health behaviors and preferences. They estimated that 43.2% of married women reported using some modern method of contraception. 

```{r nepalmm, fig.cap="Current use of contraception by age in Nepal. Source: DHS Nepal 2011, [https://tinyurl.com/y4u5wfkv](https://tinyurl.com/y4u5wfkv).", echo=F, fig.margin=FALSE, fig.fullwidth = TRUE}
knitr::include_graphics("images/dhsnepalmm.png", dpi = NA)
```

Of course this is what they learned from the *sample*, but the research question required inference to the all women in Nepal in this demographic (the *target population*). As you'll learn in **[Chapter 13](sampling)**, there is some error involved in speaking with some but not all women in Nepal, and the researchers estimated that the true percentage probably ranged from 41.0% to 45.3%.[^fastloose] This is an example of descriptive inference to answer a descriptive research question.

[^fastloose]: I'm being a bit fast and loose with the interpretation of this confidence (or uncertainty) interval, but I'll make up for it later.

### PREDICTIVE/RELATIONAL

```{marginfigure}
Of course not everyone needs to be using modern methods of contraception. If you're not sexually active, you're not at risk for pregnancy. Or if you're trying to get pregant, modern methods will make that challenging. Therefore, public health officials wanting to promote modern method use would take this indicator and combine it with several others in the dateset to estimate the ["unmet need" for family planning](https://dhsprogram.com/topics/Unmet-Need.cfm): women who say that they want to prevent or delay pregnancy, but are not using contraception.
```

Description is essential to science and decision-making related to needs and resources. The result from Nepal suggests that more than half of married women of reproductive age were not using a modern method of contraception in 2010. This is a very useful thing to know if you work for the Ministry of Health and are concerned about promoting reproductive health. 

But you probably also want to go the next step and ask, "What *predicts* modern method use?" Stated differently, what factors are *associated with/correlated with/related to* modern method use? Who is most likely to use modern methods? What are the barriers to modern method use? These are questions about the strength and direction of the relationship between two or more variables and represent our second category of research questions.

```{r nepalmmplot, fig.cap="Predicted probabilities of use of modern method of contraception. Source: Yours truly using data from the DHS Nepal 2011 survey, [https://tinyurl.com/y4u5wfkv](https://tinyurl.com/y4u5wfkv).", echo=F, fig.margin=TRUE, fig.fullwidth = FALSE}
knitr::include_graphics("images/modernprob.png", dpi = NA)
```


If you inspect the above figure from Nepal once again, you will see that the cross-tabulation of `any modern method` by `age` shows that modern method use is more common among older women compared to younger women. Modern method use appears to increase with age, according to this simple descriptive summary.

So we might want to ask, "To what extent does age predict self-reported modern method usage among currently married women of reproductive age in Nepal?" We can answer this question with a statistical model called logistic regression. The figure shows the predicted probabilities of modern method use by age among currently married women. As we expected, modern method use is more common among older women. 

### CAUSAL

```{r matrix, fig.cap="Unfortunately, no one can be told what the Matrix is. You have to see it for yourself. This is your last chance. After this there is no turning back. You take the blue pill, the story ends, you wake up in your bed and believe whatever you want to believe. You take the red pill, you stay in Wonderland, and I show you how deep the rabbit hole goes...", echo=F, fig.margin=TRUE, fig.fullwidth = FALSE}
knitr::include_graphics("images/Matrix_Blue_or_Red_Pill.gif", dpi = NA)
```

Questions about the relationship between `X` and `Y` make up the bulk of literature in global health. However, sometimes we want to go beyond asking to what degree `X` and `Y` are related and ask, "Does `X` *cause* `Y`?", or "What is the *causal effect* of `X` on `Y`?" These are causal questions. They reflect our desire to know whether our treatments, policies, and programs promote behavior change and make people healthier. I'll introduce you to this topic of causal inference in more detail in **[Chapter 8](inference)**, but let's consider for a moment the nature of a causal question.

To answer a causal question, we must think "what if". If you want to determine whether a new drug helps people recover faster from a disease, a fundamental problem you'll face is that you can't give this new drug *and* the old drug to the *same* person *simultaneously*. Once this person goes down the new drug path, they can't go down the old drug path (at the same time). So we have to create a situation where we can ask, "What would have happened if this person had gone down the old drug path instead?" This alternate path is known as the **counterfactual**, and we'll discuss it much more depth as we go. For now, I'll simply describe it as the path not taken.

```{marginfigure}
Research in low-income countries has estimated that nearly all women want to prevent or space their next pregnancy after they have just given birth, but [6 in 10 women do not start a method of family planning after delivering a baby](https://www.familyplanning2020.org/ppfp). Precisely when women become fertile after childbirth varies and is influenced by factors like breastfeeding, but in most cases ovulation returns before family planning is started. This puts women at risk of getting pregnant again before they want to. One option is to have an IUD inserted immediately after delivery of the placenta or within the first month postpartum.
```

Consider another example from Nepal. @pradhan2019 looked at the path not taken for counseling women on the benefits of having an intra-uterine device (IUD) inserted following childbirth to prevent a new pregnancy. The authors used a stepped-wedge design that you'll meet in **[Chapter 9](experimental)** to estimate the causal effect of offering postpartum family planning counseling on the proportion of new moms who opt to have an IUD inserted. By this design, the counseling intervention was 'turned on' in six hospitals in a stepwise fashion. The three hospitals in Group 1 received the intervention first in a staggered start. Approximately six months later, the remaining hospitals began offering the counseling intervention, one after the other. This figure demonstrates that rates of uptake appeared to jump in each group after the intervention was introduced. @pradhan2019 estimated that the intervention increased IUD uptake by 4.4  percentage points [95%CI: 2.8–6.4 pp].

```{r ppiud, fig.cap="Trends in PPIUD uptake. Source: @pradhan2019.", echo=F, fig.margin=FALSE, fig.fullwidth = TRUE}
knitr::include_graphics("images/ppiud.png", dpi = NA)
```

On the surface, questions of causal inference (or counterfactual prediction if you prefer) resemble questions of prediction/relation.[^predictivemodel] For `X` to cause `Y`, it must be true that `X` is related to or associated with `Y`.

What distinguishes these questions is often the research design we use to find answers, so that is where we will spend our time. And it will be time well-spent. Causal inference is one of the most important topics for applied global health researchers. When it comes to making decisions about prevention and treatment and how to allocate resources, we want to know what works, why, and for whom. 

[^predictivemodel]: See [this discussion on Andrew Gelman's blog](https://statmodeling.stat.columbia.edu/2019/06/18/causal-inference-i-recommend-the-classical-approach-an-observational-study-is-understood-in-reference-to-a-hypothetical-controlled-experiment/) where he frames causal models as a special case of predictive models.

## Specifying your research question

```{marginfigure}
<iframe width="300" height="169" src="https://www.youtube.com/embed/_0HxMpJsm0I" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> Need to develop a good qualitative research question? See this short video by Yale University (2015). Fundamentals of Qualitative Research Methods: Developing a Qualitative Research Question (Module 2). Source: [https://tinyurl.com/y5sexqwg](https://tinyurl.com/y5sexqwg).
```

A good research question is a specific research question. I'll share two mneumonics to help you get specific: FINER and PICO. 

Imagine that we wanted to study the uptake or use of bed nets. We could ask a descriptive research question like, "How many children sleep under bed nets?" It's a good start, but this question is too general. Children of what age? Living where? We also need to define what we mean by sleeping under a bed net. In this line of research, it is common to ask about the previous night, as in the night before the survey. 

A better way to phrase the question is, "What percentage of children under 5 years of age in Kenya slept under an insecticide treated net the previous night?" An example of a predictive research question on the same topic is, "What are the predictors of the use of insecticide treated net among children under 5 years of age in Kenya?"

Both of these examples are FINER than the first one: Feasible, Interesting, Novel, Ethical, and Relevant [@hully:2007].

<br>

```{r finer, echo=FALSE}
  fine <- data.frame(letter=c("**Feasible**", "**Interesting**", 
                              "**Novel**", "**Ethical**", 
                              "**Relevant**"),
                     label=c("Some resarch questions will take a long time to answer, cost too much, require too many participants, require skills or equipment that you do not have, or will be too complex to implement.",
                             "Research requires funding and effort. If you do not ask a sufficiently interesting question, you will not get funding. If you manage to get funding but lose interest in the question, you might not finish. Unlike other domains, global health research tends to have long timelines, and it's important to work on things you will find interesting over the long term.",
                             "Replication is an important part of science, but the majority of funding goes to research that asks new and interesting questions.",
                             "It would be very interesting to create a prison simulation to determine whether charactristics of the people or situation cause abusive behavior, but this would not be ethical because it could lead to the harmful treatment of research subjects. [Right?](https://en.wikipedia.org/wiki/Stanford_prison_experiment)",
                             "In addition to being interesting, a research question should also be relevant. The answer should move the field forward in some way. Making this determination requires a thorough review of the literature and conversations with senior colleagues."))
  names(fine) <- NULL
  knitr::kable(fine, col.names=NA, format = "html", 
caption = 'FINER research questions.'
) %>%
  html_table_width(c(100,500)) %>%
  kable_styling(bootstrap_options = c("striped"), full_width = FALSE, position = "left")
```

<br>

The second pneumonic is PICO, which I introduced in **[Chapter 3](filtered)**. In clinical research, good clinical questions always include [PICO](http://guides.mclibrary.duke.edu/c.php?g=158178&p=1035882): Patient/Population/Problem, Intervention/Prognostic factor/Exposure, Comparison, and Outcome.

<br>

```{r picotbl, echo=FALSE}
  picotbl <- data.frame(letter=c("**P**", "**I**", "**C**", "**O**"),
                     label=c("Patient, Population, or Problem",
                             "Intervention, Prognostic Factor, or Exposure",
                             "Comparison",
                             "Outcome"))
  names(picotbl) <- NULL
  knitr::kable(picotbl, col.names=NA, format = "html", 
caption = 'PICO research questions.'
) %>%
  html_table_width(c(100,500)) %>%
  kable_styling(bootstrap_options = c("striped"), full_width = FALSE, position = "left")
```

<br>

Let's use PICO to develop a research question about the efficacy of mosquito bed nets in preventing malaria. The **problem** is malaria infections. The **population** is children under 5 years of age. Because intervention studies tend to be smaller in reach than nationally representative surveys, we might add "living around the Lake Victoria basin in Kenya". The **intervention** is the application of an insecticide-treated net. The comparison group might be children living in families who are provided an untreated bed net.[^prog] One **outcome** measure could be the rate of parasitaemia after the intervention.

[^prog]: Prognostic factor refers to covariates that could influence the prognosis of the patient. An exposure would be something that we think might increase the risk of an outcome.

We combine all of these elements into a single research question: 

> Among children under 5 years of age living around the Lake Victoria basin in Kenya, are insecticide-treated mosquito nets more effective than untreated nets at preventing parasitaemia?

## Developing hypotheses

```{r bird, fig.cap="Can you see yourself in [little Jimmy](https://knowyourmeme.com/photos/204893-fake-science-fair-projects)?", echo=F, fig.margin=FALSE, fig.fullwidth = FALSE}
knitr::include_graphics("images/birdposter.jpg", dpi = NA)
```

Chances are you've presented a poster like this at a science fair at some point in your life. It depicts the scientific method in action:

1. Define a question
2. Develop a hypothesis
3. Collect data related to this hypothesis
4. Analyze the data 
5. Make conclusions based on the data
6. Replicate the study 

Some people call this the hypothetico-deductive model because we develop a ***hypothesis*** (a conjecture) and *deduce* the consequences that should follow from this hypothesis. Stating what we expect based on a hypothesis makes it falsifiable. We can collect data and determine whether the data are consistent with or opposed to this prediction. If the data conflict with the hypothesis, we *reject* the hypothesis. If the data are consistent with the hypothesis, we say that we have 'corroborated' our hypothesis. In this model of hypothesis testing, data never 'prove' a hypothesis, and we never 'accept' our hypothesis because it's possible that new data could falsify it. 

```{marginfigure}
[Daniël Lakens](https://twitter.com/lakens), the creator of the excellent Coursera course, "[Improving your statistical inferences](https://www.coursera.org/learn/statistical-inferences/home/info)", turned me on to a great book called "[Understanding Psychology as a Science](https://amzn.to/2RthGEH)" by @dienes2008. I highly recommend this book (and Lakens' course) to build a strong foundation in your own philosophy of science.
```

This characteristic of falsifiability is what the famous philosopher of science [Karl Popper](https://en.wikipedia.org/wiki/Karl_Popper) said distinguishes science from non-science. Science advances as theories are subjected to tests in which we make specific, falsifiable predictions based on these theories. As we will see, not all tests are 'severe' and it's possible to proceed with a falsificiation-lite approach that has the veneer of science, but is empty at its core. 

### WHERE DO HYPOTHESES COME FROM?

In the classical view of science, we develop hypotheses out of existing **theory**, an explanation of some aspect of our world that has been confirmed through repeated, falsifiable tests. We'll discuss the role of theory in global health in the following chapter, so for now let it suffice to say that theories can be a good source of hypotheses.

```{marginfigure}
Pro tip: If you're looking to explore or describe some phenomenon and are struggling to identify a testable hypothesis, you can probably stop trying. You might be operating in a hypothesis-*generating* framework, rather than a hypothesis-*testing* framework.
<br>
<br>
It's also worth noting that a study can include elements of both qualitative and quantitive approaches. We call this **mixed methods research**, and it's a topic we'll cover in **[Chapter 15](qualitative)**.
```

We can also begin with our observations of the world and generate hypotheses to test. This can be an informal process, or it can take the form of formal qualitative research. Whereas quantitative studies are often deductive—state a hypothesis, deduce the consequences that would be consistent with the hypothesis, and collect data to test this hypothesis—qualitative studies are more often inductive or exploratory. In qualitative research, we make observations about the world and use these observations to describe why or how the world appears to operate the way it does. From these observations we can create testable hypotheses for future studies that, if not rejected, can generate theory. 

### THE STRUCTURE OF HYPOTHESES

Let's return to our example research question from above:

> Among children under 5 years of age living around the Lake Victoria basin in Kenya, are insecticide-treated mosquito nets more effective than untreated nets at preventing parasitaemia?

A question like this leads to a common type of hypothesis in global health: the new treatment (insecticide-treated nets) will be superior to   the existing treatment (untreated nets). When describing your study, it's common to state a **scientific (or research) hypothesis** like this:

```{marginfigure}
This is a falsifiable hypothesis because if we found that children who slept under treated bed nets were more likely to exhibit parasitaemia, it would lead us to reject the notion that treated bed nets are superior untreated bed nets.
```

> Sleeping under an insecticide-treated bed net is more effective at preventing pediatric malaria compared to sleeping under an untreated net.

But this is not actually the hypothesis that we test.

In hypothesis testing, we set up two precise **scientific hypotheses**: a null hypothesis (H~0~) and an alternative hypothesis (H~1~). Most often the null hypothesis is stated as the hypothesis of no difference:

```{marginfigure}
The "null" hypothesis doesn't have to be a hypothesis of no difference [@dienes2008]. The null could be a specific difference that is a value other than 0, e.g., μ~1~ `-` μ~2~ `=` 3, or a band of differences.
```

> H~0~: μ~1~ `=` μ~2~ (or μ~1~ `-` μ~2~ `=` 0), meaning there is no difference in observed cases of parasitaemia between the kids who sleep under a treated bed net and those who sleep under an untreated bed net

```{marginfigure}
In this 'prevention of a bad thing' framing, "superior" means observing fewer cases of parasitaemia.
```

> H~1~: μ~1~ `<` μ~2~ (or μ~1~ `-` μ~2~ `<` 0), meaning we observe fewer cases of parasitaemia among the kids who sleep under a treated bed net compared to those who sleep under an untreated bed net

It might seem confusing because H~1~ is the hypothesis we talk about and write about, but it's actually the null hypothesis (H~0~) that we test and decide to reject or accept (technically, 'fail to reject'). We'll come back to hypothesis testing (and other goals we might have) in **[Chapter 8](causal)**. 

### WHEN ARE HYPOTHESES DEVELOPED?

```{r hark, fig.cap="The hypothetico-deductive model of the scientific method is short-circuited by a range of questionable research practices (red). HARKing, or hypothesizing after results are known, involves generating a hypothesis from the data and then presenting it as a priori. Source: [https://cos.io/rr/](https://cos.io/rr/)", echo=F, fig.margin=TRUE, fig.fullwidth = FALSE}
knitr::include_graphics("images/hark.png", dpi = NA)
```

This probably seems like a redundant question because I already listed the order of the scientific method, and I acknowledged that you probably learned this order first hand in primary school. But here's the thing: scientists sometimes Hypothesize After the Results are Known. This is known as **HARKing**, and it's a no no [@kerr1998].

To be clear: it's 100% OK to collect data, analyze the results, and generate new ideas (hypotheses) to test in future studies. That's called exploratory analysis, and if you label it as such everyone is happy.

But it's not OK—in fact it's dishonest and a form of fraud—to look at your data, develop hypotheses that conform to the data, and pass your new ideas off as *a priori* hypotheses that you tested. That's lying. And it's not science. You can't falsify a hypothesis that you create after looking at the data.

I'm going to refrain from hitting you with some depressive statistics about the state of science today. It's only Chapter 5. I'm still working on building your enthusiasm for research. We'll revisit research sins like HARKing in **[Chapter 18](openscience)**, along with strategies for preventing such bad behavior. 

## Manuscripts: Writing the Introduction 

Let's take a look at how research problems, questions, and hypotheses come together in the *Introduction* of a scientific manuscript. Take a moment and [download this article](https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(18)30126-8/fulltext) in *The Lancet Global Health* by @staedke2018. I've highlighted the authors' descriptions of the research problem, what is known about the problem, the research question, and the hypothesis.

```{r startipt, fig.cap="Example *Introduction* section published in *The Lancet Global Health*. Source: @staedke2018", echo=F, fig.margin=FALSE, fig.fullwidth = TRUE}
knitr::include_graphics("images/startipt.png", dpi = NA)
```

The research problem is that malaria control remains a persistent challenge and existing remedies are expensive and hard to sustain. We know that [intermittent preventive treatment](https://en.wikipedia.org/wiki/Intermittent_preventive_therapy) has benefits to individuals, but there is little evidence of community-level benefits.

The authors framed the objective of the study as follows: 

> In the School-based Treatment with ACTs to Reduce Transmission of malaria (START-IPT) trial, we investigated whether IPT for malaria with dihydroartemisinin-piperaquine (DP) in schoolchildren would affect community-level indicators of malaria transmission in Jinja district, Uganda.

We can work backwards (with a little help from the hypothesis) and use PICO to construct the implied research question:

```{r picoipt, echo=FALSE}
  picoipt <- data.frame(label=c("Population",
                             "Intervention",
                             "Comparison",
                             "Outcome"),
                       details=c("schoolchildren in Jinga, Uganda", "IPT with dihydroartemisinin-piperaquine", "no IPT", "community-level indicators of malaria transmission (i.e., prevalence of asexual parasitaemia in the community and the annual entomological inoculation rate)"))
  names(picoipt) <- NULL
  knitr::kable(picoipt, col.names=NA, format = "html", 
caption = 'Reframing the research question in @staedke2018.'
) %>%
  html_table_width(c(100,500)) %>%
  kable_styling(bootstrap_options = c("striped"), full_width = FALSE, position = "left")
```

<br>

> Among schoolchildren in Jinga, Uganda, does IPT with dihydroartemisinin-piperaquine reduce the prevalence of asexual parasitaemia in the community and increase the annual entomological inoculation rate compared to no IPT controls?

If you're coming from a discipline like economics, you might wonder if I forgot to paste the full *Introduction* of @staedke2018. I did not. This is it. It's common in the health sciences to publish very brief *Introduction* sections. 

For comparison, here's an example *Introduction* from @blattman2017 published in *American Economic Review*. The first thing you notice is the difference in length. The complete typeset paper is 42 pages, compared to only 12 pages for the above example published in *The Lancet Global Health*. The *AER* article is gated, but if you [download the pre-print](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2594868) and read the *Introduction*, you'll also get a sense for how economists structure scientific papers differently. 

```{r liberiacbt, fig.cap="Example *Introduction* section published in *American Economic Review*. Source: @blattman2017", echo=F, fig.margin=FALSE, fig.fullwidth = TRUE}
knitr::include_graphics("images/liberiacbt.png", dpi = NA)
```

## Proposals: Writing Specific Aims

While most scientific papers follow the pattern of describing the research problem and outlining the objectives of the paper, you will find that it's relatively uncommon, at least in the health sciences, to explicitly state the research question as a question. If you look at recent issues of a journal like [*The Lancet Global Health*](https://www.thelancet.com/journals/langlo/home), you'll also find that many articles do not even state explicit hypotheses.  

TO ADD

## The Takeaway

TO ADD

